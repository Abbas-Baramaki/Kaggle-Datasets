{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-31T07:31:20.894531Z","iopub.execute_input":"2023-10-31T07:31:20.894932Z","iopub.status.idle":"2023-10-31T07:31:20.900855Z","shell.execute_reply.started":"2023-10-31T07:31:20.894901Z","shell.execute_reply":"2023-10-31T07:31:20.899582Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"### https://www.kaggle.com/datasets/primaryobjects/voicegender","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/voicegender/voice.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:20.916363Z","iopub.execute_input":"2023-10-31T07:31:20.917105Z","iopub.status.idle":"2023-10-31T07:31:20.965705Z","shell.execute_reply.started":"2023-10-31T07:31:20.917059Z","shell.execute_reply":"2023-10-31T07:31:20.964596Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:20.967922Z","iopub.execute_input":"2023-10-31T07:31:20.968715Z","iopub.status.idle":"2023-10-31T07:31:21.016133Z","shell.execute_reply.started":"2023-10-31T07:31:20.968654Z","shell.execute_reply":"2023-10-31T07:31:21.014638Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n...        ...       ...       ...       ...       ...       ...        ...   \n3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n\n             kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n0      274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n1      634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n2     1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n3        4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n4        4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n...           ...       ...       ...  ...       ...       ...       ...   \n3163     6.630383  0.962934  0.763182  ...  0.131884  0.182790  0.083770   \n3164     2.503954  0.960716  0.709570  ...  0.116221  0.188980  0.034409   \n3165     6.604509  0.946854  0.654196  ...  0.142056  0.209918  0.039506   \n3166     5.388298  0.950436  0.675470  ...  0.143659  0.172375  0.034483   \n3167     5.769115  0.938829  0.601529  ...  0.165509  0.185607  0.062257   \n\n        maxfun   meandom    mindom    maxdom   dfrange   modindx   label  \n0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000    male  \n1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632    male  \n2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512    male  \n3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119    male  \n4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274    male  \n...        ...       ...       ...       ...       ...       ...     ...  \n3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929  female  \n3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897  female  \n3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759  female  \n3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002  female  \n3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000  female  \n\n[3168 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>meanfreq</th>\n      <th>sd</th>\n      <th>median</th>\n      <th>Q25</th>\n      <th>Q75</th>\n      <th>IQR</th>\n      <th>skew</th>\n      <th>kurt</th>\n      <th>sp.ent</th>\n      <th>sfm</th>\n      <th>...</th>\n      <th>centroid</th>\n      <th>meanfun</th>\n      <th>minfun</th>\n      <th>maxfun</th>\n      <th>meandom</th>\n      <th>mindom</th>\n      <th>maxdom</th>\n      <th>dfrange</th>\n      <th>modindx</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.059781</td>\n      <td>0.064241</td>\n      <td>0.032027</td>\n      <td>0.015071</td>\n      <td>0.090193</td>\n      <td>0.075122</td>\n      <td>12.863462</td>\n      <td>274.402906</td>\n      <td>0.893369</td>\n      <td>0.491918</td>\n      <td>...</td>\n      <td>0.059781</td>\n      <td>0.084279</td>\n      <td>0.015702</td>\n      <td>0.275862</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.007812</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.066009</td>\n      <td>0.067310</td>\n      <td>0.040229</td>\n      <td>0.019414</td>\n      <td>0.092666</td>\n      <td>0.073252</td>\n      <td>22.423285</td>\n      <td>634.613855</td>\n      <td>0.892193</td>\n      <td>0.513724</td>\n      <td>...</td>\n      <td>0.066009</td>\n      <td>0.107937</td>\n      <td>0.015826</td>\n      <td>0.250000</td>\n      <td>0.009014</td>\n      <td>0.007812</td>\n      <td>0.054688</td>\n      <td>0.046875</td>\n      <td>0.052632</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.077316</td>\n      <td>0.083829</td>\n      <td>0.036718</td>\n      <td>0.008701</td>\n      <td>0.131908</td>\n      <td>0.123207</td>\n      <td>30.757155</td>\n      <td>1024.927705</td>\n      <td>0.846389</td>\n      <td>0.478905</td>\n      <td>...</td>\n      <td>0.077316</td>\n      <td>0.098706</td>\n      <td>0.015656</td>\n      <td>0.271186</td>\n      <td>0.007990</td>\n      <td>0.007812</td>\n      <td>0.015625</td>\n      <td>0.007812</td>\n      <td>0.046512</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.151228</td>\n      <td>0.072111</td>\n      <td>0.158011</td>\n      <td>0.096582</td>\n      <td>0.207955</td>\n      <td>0.111374</td>\n      <td>1.232831</td>\n      <td>4.177296</td>\n      <td>0.963322</td>\n      <td>0.727232</td>\n      <td>...</td>\n      <td>0.151228</td>\n      <td>0.088965</td>\n      <td>0.017798</td>\n      <td>0.250000</td>\n      <td>0.201497</td>\n      <td>0.007812</td>\n      <td>0.562500</td>\n      <td>0.554688</td>\n      <td>0.247119</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.135120</td>\n      <td>0.079146</td>\n      <td>0.124656</td>\n      <td>0.078720</td>\n      <td>0.206045</td>\n      <td>0.127325</td>\n      <td>1.101174</td>\n      <td>4.333713</td>\n      <td>0.971955</td>\n      <td>0.783568</td>\n      <td>...</td>\n      <td>0.135120</td>\n      <td>0.106398</td>\n      <td>0.016931</td>\n      <td>0.266667</td>\n      <td>0.712812</td>\n      <td>0.007812</td>\n      <td>5.484375</td>\n      <td>5.476562</td>\n      <td>0.208274</td>\n      <td>male</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3163</th>\n      <td>0.131884</td>\n      <td>0.084734</td>\n      <td>0.153707</td>\n      <td>0.049285</td>\n      <td>0.201144</td>\n      <td>0.151859</td>\n      <td>1.762129</td>\n      <td>6.630383</td>\n      <td>0.962934</td>\n      <td>0.763182</td>\n      <td>...</td>\n      <td>0.131884</td>\n      <td>0.182790</td>\n      <td>0.083770</td>\n      <td>0.262295</td>\n      <td>0.832899</td>\n      <td>0.007812</td>\n      <td>4.210938</td>\n      <td>4.203125</td>\n      <td>0.161929</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3164</th>\n      <td>0.116221</td>\n      <td>0.089221</td>\n      <td>0.076758</td>\n      <td>0.042718</td>\n      <td>0.204911</td>\n      <td>0.162193</td>\n      <td>0.693730</td>\n      <td>2.503954</td>\n      <td>0.960716</td>\n      <td>0.709570</td>\n      <td>...</td>\n      <td>0.116221</td>\n      <td>0.188980</td>\n      <td>0.034409</td>\n      <td>0.275862</td>\n      <td>0.909856</td>\n      <td>0.039062</td>\n      <td>3.679688</td>\n      <td>3.640625</td>\n      <td>0.277897</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3165</th>\n      <td>0.142056</td>\n      <td>0.095798</td>\n      <td>0.183731</td>\n      <td>0.033424</td>\n      <td>0.224360</td>\n      <td>0.190936</td>\n      <td>1.876502</td>\n      <td>6.604509</td>\n      <td>0.946854</td>\n      <td>0.654196</td>\n      <td>...</td>\n      <td>0.142056</td>\n      <td>0.209918</td>\n      <td>0.039506</td>\n      <td>0.275862</td>\n      <td>0.494271</td>\n      <td>0.007812</td>\n      <td>2.937500</td>\n      <td>2.929688</td>\n      <td>0.194759</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3166</th>\n      <td>0.143659</td>\n      <td>0.090628</td>\n      <td>0.184976</td>\n      <td>0.043508</td>\n      <td>0.219943</td>\n      <td>0.176435</td>\n      <td>1.591065</td>\n      <td>5.388298</td>\n      <td>0.950436</td>\n      <td>0.675470</td>\n      <td>...</td>\n      <td>0.143659</td>\n      <td>0.172375</td>\n      <td>0.034483</td>\n      <td>0.250000</td>\n      <td>0.791360</td>\n      <td>0.007812</td>\n      <td>3.593750</td>\n      <td>3.585938</td>\n      <td>0.311002</td>\n      <td>female</td>\n    </tr>\n    <tr>\n      <th>3167</th>\n      <td>0.165509</td>\n      <td>0.092884</td>\n      <td>0.183044</td>\n      <td>0.070072</td>\n      <td>0.250827</td>\n      <td>0.180756</td>\n      <td>1.705029</td>\n      <td>5.769115</td>\n      <td>0.938829</td>\n      <td>0.601529</td>\n      <td>...</td>\n      <td>0.165509</td>\n      <td>0.185607</td>\n      <td>0.062257</td>\n      <td>0.271186</td>\n      <td>0.227022</td>\n      <td>0.007812</td>\n      <td>0.554688</td>\n      <td>0.546875</td>\n      <td>0.350000</td>\n      <td>female</td>\n    </tr>\n  </tbody>\n</table>\n<p>3168 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.018586Z","iopub.execute_input":"2023-10-31T07:31:21.019067Z","iopub.status.idle":"2023-10-31T07:31:21.038553Z","shell.execute_reply.started":"2023-10-31T07:31:21.019021Z","shell.execute_reply":"2023-10-31T07:31:21.037109Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3168 entries, 0 to 3167\nData columns (total 21 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   meanfreq  3168 non-null   float64\n 1   sd        3168 non-null   float64\n 2   median    3168 non-null   float64\n 3   Q25       3168 non-null   float64\n 4   Q75       3168 non-null   float64\n 5   IQR       3168 non-null   float64\n 6   skew      3168 non-null   float64\n 7   kurt      3168 non-null   float64\n 8   sp.ent    3168 non-null   float64\n 9   sfm       3168 non-null   float64\n 10  mode      3168 non-null   float64\n 11  centroid  3168 non-null   float64\n 12  meanfun   3168 non-null   float64\n 13  minfun    3168 non-null   float64\n 14  maxfun    3168 non-null   float64\n 15  meandom   3168 non-null   float64\n 16  mindom    3168 non-null   float64\n 17  maxdom    3168 non-null   float64\n 18  dfrange   3168 non-null   float64\n 19  modindx   3168 non-null   float64\n 20  label     3168 non-null   object \ndtypes: float64(20), object(1)\nmemory usage: 519.9+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"y = data['label']\nx = data.drop('label',axis=1).copy()\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\ny = y.apply(lambda x: 1 if x=='male' else 0)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.040313Z","iopub.execute_input":"2023-10-31T07:31:21.040794Z","iopub.status.idle":"2023-10-31T07:31:21.063749Z","shell.execute_reply.started":"2023-10-31T07:31:21.040750Z","shell.execute_reply":"2023-10-31T07:31:21.062483Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"y = y.apply(lambda x: 1 if x=='male' else 0)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.066288Z","iopub.execute_input":"2023-10-31T07:31:21.066684Z","iopub.status.idle":"2023-10-31T07:31:21.076553Z","shell.execute_reply.started":"2023-10-31T07:31:21.066651Z","shell.execute_reply":"2023-10-31T07:31:21.075221Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nx = scaler.fit_transform(x)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.078721Z","iopub.execute_input":"2023-10-31T07:31:21.079191Z","iopub.status.idle":"2023-10-31T07:31:21.087351Z","shell.execute_reply.started":"2023-10-31T07:31:21.079149Z","shell.execute_reply":"2023-10-31T07:31:21.085953Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.089607Z","iopub.execute_input":"2023-10-31T07:31:21.090065Z","iopub.status.idle":"2023-10-31T07:31:21.101301Z","shell.execute_reply.started":"2023-10-31T07:31:21.090025Z","shell.execute_reply":"2023-10-31T07:31:21.099994Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"(3168, 20)"},"metadata":{}}]},{"cell_type":"code","source":"x_train , x_test , y_train , y_test = train_test_split(x,y,train_size=0.7,random_state=13)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.105228Z","iopub.execute_input":"2023-10-31T07:31:21.106242Z","iopub.status.idle":"2023-10-31T07:31:21.117611Z","shell.execute_reply.started":"2023-10-31T07:31:21.106197Z","shell.execute_reply":"2023-10-31T07:31:21.116726Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(x.shape[1]))\n\nx = tf.keras.layers.Dense(64,activation='relu')(inputs)\nx = tf.keras.layers.Dense(128,activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs,outputs=outputs)\n\nmodel.compile(\n    optimizer='adam'\n    ,loss='binary_crossentropy'\n    ,metrics=['accuracy',tf.keras.metrics.AUC(name='auc')]\n)\n\nhist = model.fit(\n    x_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=35,\n    epochs=100,\n    callbacks=tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss'\n        ,patience=10\n        ,restore_best_weights=True)\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:21.155260Z","iopub.execute_input":"2023-10-31T07:31:21.156060Z","iopub.status.idle":"2023-10-31T07:31:42.729821Z","shell.execute_reply.started":"2023-10-31T07:31:21.156024Z","shell.execute_reply":"2023-10-31T07:31:42.728894Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Epoch 1/100\n51/51 [==============================] - 1s 9ms/step - loss: 0.1341 - accuracy: 0.9848 - auc: 0.0000e+00 - val_loss: 0.0089 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 2/100\n51/51 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 0.0022 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 3/100\n51/51 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 9.7284e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 4/100\n51/51 [==============================] - 0s 3ms/step - loss: 7.1406e-04 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.5868e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 5/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.3066e-04 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.5600e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 6/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.8654e-04 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.4736e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 7/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.0410e-04 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.8314e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 8/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.5309e-04 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.3821e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 9/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.1850e-04 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.0966e-04 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 10/100\n51/51 [==============================] - 0s 3ms/step - loss: 9.4832e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 8.8383e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 11/100\n51/51 [==============================] - 0s 3ms/step - loss: 7.7521e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.2373e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 12/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.4280e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.1316e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 13/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.4361e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.1787e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 14/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.6435e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.4444e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 15/100\n51/51 [==============================] - 0s 4ms/step - loss: 4.0126e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.8389e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 16/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.4991e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.3638e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 17/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.0767e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.9829e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 18/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.7302e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.6393e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 19/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.4319e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.3570e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 20/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.1783e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.1172e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 21/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.9630e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.9090e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 22/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.7767e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.7336e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 23/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.6166e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.5754e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 24/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.4740e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.4400e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 25/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.3503e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.3165e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 26/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.2387e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.2130e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 27/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.1423e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.1165e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 28/100\n51/51 [==============================] - 0s 4ms/step - loss: 1.0545e-05 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.0353e-05 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 29/100\n51/51 [==============================] - 0s 3ms/step - loss: 9.7724e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 9.5946e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 30/100\n51/51 [==============================] - 0s 3ms/step - loss: 9.0715e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 8.8947e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 31/100\n51/51 [==============================] - 0s 3ms/step - loss: 8.4356e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 8.2663e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 32/100\n51/51 [==============================] - 0s 3ms/step - loss: 7.8601e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.7114e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 33/100\n51/51 [==============================] - 0s 3ms/step - loss: 7.3418e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.2053e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 34/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.8687e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.7254e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 35/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.4311e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.3052e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 36/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.0329e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.9287e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 37/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.6680e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.5874e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 38/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.3377e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.2371e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 39/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.0260e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.9302e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 40/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.7374e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.6588e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 41/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.4756e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.3989e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 42/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.2272e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.1542e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 43/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.9993e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.9347e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 44/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.7916e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.7192e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 45/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.5913e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.5272e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 46/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.4079e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.3532e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 47/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.2381e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.1822e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 48/100\n51/51 [==============================] - 0s 3ms/step - loss: 3.0764e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.0237e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 49/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.9267e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.8742e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 50/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.7854e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.7363e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 51/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.6537e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.6039e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 52/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.5281e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.4868e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 53/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.4130e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.3681e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 54/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.3025e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.2570e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 55/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.1977e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.1600e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 56/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.1006e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 2.0625e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 57/100\n51/51 [==============================] - 0s 3ms/step - loss: 2.0079e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.9705e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 58/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.9205e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.8827e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 59/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.8378e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.8031e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 60/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.7595e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.7275e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 61/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.6861e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.6541e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 62/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.6153e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.5868e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 63/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.5496e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.5194e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 64/100\n51/51 [==============================] - 0s 4ms/step - loss: 1.4858e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.4587e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 65/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.4256e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.3988e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 66/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.3687e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.3420e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 67/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.3138e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.2899e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 68/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.2624e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.2378e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 69/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.2129e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.1891e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 70/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.1661e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.1428e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 71/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.1209e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.1009e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 72/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.0786e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.0594e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 73/100\n51/51 [==============================] - 0s 3ms/step - loss: 1.0382e-06 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 1.0170e-06 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 74/100\n51/51 [==============================] - 0s 3ms/step - loss: 9.9872e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 9.7853e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 75/100\n51/51 [==============================] - 0s 4ms/step - loss: 9.6099e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 9.4385e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 76/100\n51/51 [==============================] - 0s 3ms/step - loss: 9.2593e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 9.0729e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 77/100\n51/51 [==============================] - 0s 3ms/step - loss: 8.9154e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 8.7431e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 78/100\n51/51 [==============================] - 0s 4ms/step - loss: 8.5917e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 8.4263e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 79/100\n51/51 [==============================] - 0s 3ms/step - loss: 8.2762e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 8.1202e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 80/100\n51/51 [==============================] - 0s 4ms/step - loss: 7.9796e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.8109e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 81/100\n51/51 [==============================] - 0s 3ms/step - loss: 7.6882e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.5325e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 82/100\n51/51 [==============================] - 0s 3ms/step - loss: 7.4162e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.2640e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 83/100\n51/51 [==============================] - 0s 4ms/step - loss: 7.1544e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 7.0045e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 84/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.8980e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.7630e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 85/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.6589e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.5190e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 86/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.4235e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.3089e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 87/100\n51/51 [==============================] - 0s 3ms/step - loss: 6.2034e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 6.0754e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 88/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.9874e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.8650e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 89/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.7832e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.6552e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 90/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.5817e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.4728e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 91/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.3926e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.2903e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 92/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.2106e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 5.1120e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 93/100\n51/51 [==============================] - 0s 3ms/step - loss: 5.0352e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.9275e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 94/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.8637e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.7579e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 95/100\n51/51 [==============================] - 0s 4ms/step - loss: 4.6983e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.6065e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 96/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.5440e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.4501e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 97/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.3943e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.2975e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 98/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.2475e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.1561e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 99/100\n51/51 [==============================] - 0s 3ms/step - loss: 4.1074e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 4.0198e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\nEpoch 100/100\n51/51 [==============================] - 0s 4ms/step - loss: 3.9729e-07 - accuracy: 1.0000 - auc: 0.0000e+00 - val_loss: 3.8870e-07 - val_accuracy: 1.0000 - val_auc: 0.0000e+00\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.731917Z","iopub.execute_input":"2023-10-31T07:31:42.732250Z","iopub.status.idle":"2023-10-31T07:31:42.863630Z","shell.execute_reply.started":"2023-10-31T07:31:42.732221Z","shell.execute_reply":"2023-10-31T07:31:42.862485Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"30/30 [==============================] - 0s 2ms/step - loss: 4.1206e-07 - accuracy: 1.0000 - auc: 0.0000e+00\n","output_type":"stream"},{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"[4.1206240553037787e-07, 1.0, 0.0]"},"metadata":{}}]},{"cell_type":"code","source":"y = data['label']\nx = data.drop('label',axis=1).copy()\nscaler = StandardScaler()\nx = scaler.fit_transform(x)\ny = y.apply(lambda x: 1 if x=='male' else 0)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.864867Z","iopub.execute_input":"2023-10-31T07:31:42.865193Z","iopub.status.idle":"2023-10-31T07:31:42.880793Z","shell.execute_reply.started":"2023-10-31T07:31:42.865166Z","shell.execute_reply":"2023-10-31T07:31:42.879532Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"x = tf.keras.preprocessing.sequence.pad_sequences(x,dtype=np.float,maxlen=20,padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.884193Z","iopub.execute_input":"2023-10-31T07:31:42.885078Z","iopub.status.idle":"2023-10-31T07:31:42.901538Z","shell.execute_reply.started":"2023-10-31T07:31:42.885042Z","shell.execute_reply":"2023-10-31T07:31:42.899328Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/1044518553.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  x = tf.keras.preprocessing.sequence.pad_sequences(x,dtype=np.float,maxlen=20,padding='post')\n","output_type":"stream"}]},{"cell_type":"code","source":"x = x.reshape(-1,4,5)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.902973Z","iopub.execute_input":"2023-10-31T07:31:42.903290Z","iopub.status.idle":"2023-10-31T07:31:42.909874Z","shell.execute_reply.started":"2023-10-31T07:31:42.903263Z","shell.execute_reply":"2023-10-31T07:31:42.908657Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"x = np.expand_dims(x,axis=3)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.912819Z","iopub.execute_input":"2023-10-31T07:31:42.913639Z","iopub.status.idle":"2023-10-31T07:31:42.922324Z","shell.execute_reply.started":"2023-10-31T07:31:42.913604Z","shell.execute_reply":"2023-10-31T07:31:42.920908Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"x_train , x_test , y_train , y_test = train_test_split(x,y,train_size=0.74,random_state = 13)","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.923903Z","iopub.execute_input":"2023-10-31T07:31:42.924686Z","iopub.status.idle":"2023-10-31T07:31:42.936100Z","shell.execute_reply.started":"2023-10-31T07:31:42.924648Z","shell.execute_reply":"2023-10-31T07:31:42.934956Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(4,5,1))\n\nx = tf.keras.layers.Conv2D(16,1,activation='relu')(inputs)\nx = tf.keras.layers.MaxPooling2D()(x)\nx = tf.keras.layers.Conv2D(32,1,activation='relu')(x)\nx = tf.keras.layers.MaxPooling2D()(x)\nx = tf.keras.layers.Conv2D(64,1,activation='relu')(inputs)\nx = tf.keras.layers.MaxPooling2D()(x)\nx = tf.keras.layers.Conv2D(128,1,activation='relu')(x)\nx = tf.keras.layers.MaxPooling2D()(x)\n\nx = tf.keras.layers.Flatten()(x)\noutputs = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs,outputs=outputs)\n\nmodel.compile(\n    optimizer='adam'\n    ,loss='binary_crossentropy'\n    ,metrics=['accuracy',tf.keras.metrics.AUC(name='auc')]\n)\n\nhist = model.fit(\n    x_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=35,\n    epochs=100,\n    callbacks=tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss'\n        ,patience=10\n        ,restore_best_weights=True)\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-31T07:31:42.937596Z","iopub.execute_input":"2023-10-31T07:31:42.938037Z","iopub.status.idle":"2023-10-31T07:32:08.262553Z","shell.execute_reply.started":"2023-10-31T07:31:42.937996Z","shell.execute_reply":"2023-10-31T07:32:08.261152Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Epoch 1/100\n54/54 [==============================] - 2s 10ms/step - loss: 0.6580 - accuracy: 0.6240 - auc: 0.7306 - val_loss: 0.6212 - val_accuracy: 0.7335 - val_auc: 0.8122\nEpoch 2/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.5944 - accuracy: 0.7285 - auc: 0.7974 - val_loss: 0.5510 - val_accuracy: 0.7719 - val_auc: 0.8293\nEpoch 3/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.5365 - accuracy: 0.7701 - auc: 0.8306 - val_loss: 0.5058 - val_accuracy: 0.7910 - val_auc: 0.8801\nEpoch 4/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.5073 - accuracy: 0.7909 - auc: 0.8497 - val_loss: 0.4767 - val_accuracy: 0.7932 - val_auc: 0.8827\nEpoch 5/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4855 - accuracy: 0.7920 - auc: 0.8613 - val_loss: 0.4536 - val_accuracy: 0.8252 - val_auc: 0.8935\nEpoch 6/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.4606 - accuracy: 0.8107 - auc: 0.8754 - val_loss: 0.4389 - val_accuracy: 0.8188 - val_auc: 0.8956\nEpoch 7/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.4451 - accuracy: 0.8123 - auc: 0.8830 - val_loss: 0.4209 - val_accuracy: 0.8401 - val_auc: 0.9076\nEpoch 8/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.4319 - accuracy: 0.8165 - auc: 0.8888 - val_loss: 0.4089 - val_accuracy: 0.8571 - val_auc: 0.9111\nEpoch 9/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8203 - auc: 0.8925 - val_loss: 0.3987 - val_accuracy: 0.8422 - val_auc: 0.9087\nEpoch 10/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.8229 - auc: 0.8952 - val_loss: 0.3970 - val_accuracy: 0.8443 - val_auc: 0.9117\nEpoch 11/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4119 - accuracy: 0.8251 - auc: 0.8969 - val_loss: 0.3929 - val_accuracy: 0.8465 - val_auc: 0.9128\nEpoch 12/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4116 - accuracy: 0.8213 - auc: 0.8959 - val_loss: 0.3867 - val_accuracy: 0.8465 - val_auc: 0.9125\nEpoch 13/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.8288 - auc: 0.8991 - val_loss: 0.3910 - val_accuracy: 0.8486 - val_auc: 0.9183\nEpoch 14/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4045 - accuracy: 0.8176 - auc: 0.8998 - val_loss: 0.3932 - val_accuracy: 0.8337 - val_auc: 0.9120\nEpoch 15/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4131 - accuracy: 0.8267 - auc: 0.8937 - val_loss: 0.3793 - val_accuracy: 0.8571 - val_auc: 0.9198\nEpoch 16/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.4011 - accuracy: 0.8224 - auc: 0.9003 - val_loss: 0.3746 - val_accuracy: 0.8422 - val_auc: 0.9184\nEpoch 17/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8283 - auc: 0.9060 - val_loss: 0.3755 - val_accuracy: 0.8571 - val_auc: 0.9225\nEpoch 18/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3909 - accuracy: 0.8293 - auc: 0.9066 - val_loss: 0.3764 - val_accuracy: 0.8465 - val_auc: 0.9192\nEpoch 19/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3896 - accuracy: 0.8256 - auc: 0.9067 - val_loss: 0.3665 - val_accuracy: 0.8422 - val_auc: 0.9214\nEpoch 20/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3932 - accuracy: 0.8267 - auc: 0.9038 - val_loss: 0.3710 - val_accuracy: 0.8422 - val_auc: 0.9212\nEpoch 21/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3842 - accuracy: 0.8320 - auc: 0.9096 - val_loss: 0.3775 - val_accuracy: 0.8337 - val_auc: 0.9249\nEpoch 22/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3831 - accuracy: 0.8309 - auc: 0.9087 - val_loss: 0.3621 - val_accuracy: 0.8465 - val_auc: 0.9254\nEpoch 23/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3787 - accuracy: 0.8293 - auc: 0.9122 - val_loss: 0.3611 - val_accuracy: 0.8507 - val_auc: 0.9243\nEpoch 24/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3755 - accuracy: 0.8304 - auc: 0.9129 - val_loss: 0.3639 - val_accuracy: 0.8443 - val_auc: 0.9252\nEpoch 25/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3796 - accuracy: 0.8293 - auc: 0.9104 - val_loss: 0.3674 - val_accuracy: 0.8486 - val_auc: 0.9201\nEpoch 26/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3718 - accuracy: 0.8347 - auc: 0.9141 - val_loss: 0.3618 - val_accuracy: 0.8550 - val_auc: 0.9284\nEpoch 27/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3682 - accuracy: 0.8347 - auc: 0.9172 - val_loss: 0.3706 - val_accuracy: 0.8465 - val_auc: 0.9247\nEpoch 28/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8373 - auc: 0.9152 - val_loss: 0.3559 - val_accuracy: 0.8529 - val_auc: 0.9282\nEpoch 29/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3640 - accuracy: 0.8341 - auc: 0.9179 - val_loss: 0.3522 - val_accuracy: 0.8507 - val_auc: 0.9294\nEpoch 30/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3683 - accuracy: 0.8389 - auc: 0.9155 - val_loss: 0.3612 - val_accuracy: 0.8486 - val_auc: 0.9278\nEpoch 31/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3647 - accuracy: 0.8336 - auc: 0.9177 - val_loss: 0.3633 - val_accuracy: 0.8550 - val_auc: 0.9306\nEpoch 32/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3646 - accuracy: 0.8352 - auc: 0.9175 - val_loss: 0.3569 - val_accuracy: 0.8529 - val_auc: 0.9287\nEpoch 33/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3625 - accuracy: 0.8373 - auc: 0.9183 - val_loss: 0.3499 - val_accuracy: 0.8486 - val_auc: 0.9305\nEpoch 34/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3598 - accuracy: 0.8400 - auc: 0.9196 - val_loss: 0.3676 - val_accuracy: 0.8401 - val_auc: 0.9259\nEpoch 35/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3597 - accuracy: 0.8357 - auc: 0.9191 - val_loss: 0.3479 - val_accuracy: 0.8486 - val_auc: 0.9313\nEpoch 36/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3576 - accuracy: 0.8421 - auc: 0.9203 - val_loss: 0.3533 - val_accuracy: 0.8614 - val_auc: 0.9317\nEpoch 37/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3574 - accuracy: 0.8405 - auc: 0.9205 - val_loss: 0.3484 - val_accuracy: 0.8571 - val_auc: 0.9313\nEpoch 38/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3505 - accuracy: 0.8411 - auc: 0.9236 - val_loss: 0.3542 - val_accuracy: 0.8422 - val_auc: 0.9275\nEpoch 39/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.8443 - auc: 0.9215 - val_loss: 0.3485 - val_accuracy: 0.8486 - val_auc: 0.9299\nEpoch 40/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8432 - auc: 0.9227 - val_loss: 0.3461 - val_accuracy: 0.8507 - val_auc: 0.9318\nEpoch 41/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3509 - accuracy: 0.8469 - auc: 0.9232 - val_loss: 0.3461 - val_accuracy: 0.8614 - val_auc: 0.9325\nEpoch 42/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3519 - accuracy: 0.8411 - auc: 0.9220 - val_loss: 0.3452 - val_accuracy: 0.8529 - val_auc: 0.9314\nEpoch 43/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3513 - accuracy: 0.8432 - auc: 0.9220 - val_loss: 0.3477 - val_accuracy: 0.8486 - val_auc: 0.9294\nEpoch 44/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3494 - accuracy: 0.8464 - auc: 0.9241 - val_loss: 0.3435 - val_accuracy: 0.8550 - val_auc: 0.9330\nEpoch 45/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3536 - accuracy: 0.8517 - auc: 0.9215 - val_loss: 0.3472 - val_accuracy: 0.8593 - val_auc: 0.9325\nEpoch 46/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3462 - accuracy: 0.8459 - auc: 0.9249 - val_loss: 0.3478 - val_accuracy: 0.8635 - val_auc: 0.9328\nEpoch 47/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3488 - accuracy: 0.8416 - auc: 0.9240 - val_loss: 0.3411 - val_accuracy: 0.8529 - val_auc: 0.9336\nEpoch 48/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3488 - accuracy: 0.8443 - auc: 0.9237 - val_loss: 0.3462 - val_accuracy: 0.8593 - val_auc: 0.9334\nEpoch 49/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3486 - accuracy: 0.8485 - auc: 0.9238 - val_loss: 0.3489 - val_accuracy: 0.8465 - val_auc: 0.9280\nEpoch 50/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3454 - accuracy: 0.8485 - auc: 0.9249 - val_loss: 0.3446 - val_accuracy: 0.8571 - val_auc: 0.9330\nEpoch 51/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3601 - accuracy: 0.8491 - auc: 0.9183 - val_loss: 0.3616 - val_accuracy: 0.8380 - val_auc: 0.9274\nEpoch 52/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3454 - accuracy: 0.8389 - auc: 0.9254 - val_loss: 0.3406 - val_accuracy: 0.8529 - val_auc: 0.9343\nEpoch 53/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3427 - accuracy: 0.8491 - auc: 0.9264 - val_loss: 0.3445 - val_accuracy: 0.8486 - val_auc: 0.9341\nEpoch 54/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3443 - accuracy: 0.8459 - auc: 0.9251 - val_loss: 0.3393 - val_accuracy: 0.8507 - val_auc: 0.9348\nEpoch 55/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3483 - accuracy: 0.8469 - auc: 0.9232 - val_loss: 0.3397 - val_accuracy: 0.8529 - val_auc: 0.9346\nEpoch 56/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3417 - accuracy: 0.8507 - auc: 0.9267 - val_loss: 0.3413 - val_accuracy: 0.8486 - val_auc: 0.9324\nEpoch 57/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3407 - accuracy: 0.8507 - auc: 0.9271 - val_loss: 0.3388 - val_accuracy: 0.8529 - val_auc: 0.9355\nEpoch 58/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3502 - accuracy: 0.8379 - auc: 0.9221 - val_loss: 0.3374 - val_accuracy: 0.8550 - val_auc: 0.9350\nEpoch 59/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3410 - accuracy: 0.8512 - auc: 0.9266 - val_loss: 0.3522 - val_accuracy: 0.8443 - val_auc: 0.9311\nEpoch 60/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3408 - accuracy: 0.8507 - auc: 0.9265 - val_loss: 0.3415 - val_accuracy: 0.8550 - val_auc: 0.9361\nEpoch 61/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3434 - accuracy: 0.8485 - auc: 0.9255 - val_loss: 0.3400 - val_accuracy: 0.8486 - val_auc: 0.9335\nEpoch 62/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3415 - accuracy: 0.8560 - auc: 0.9266 - val_loss: 0.3380 - val_accuracy: 0.8657 - val_auc: 0.9348\nEpoch 63/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8491 - auc: 0.9280 - val_loss: 0.3375 - val_accuracy: 0.8593 - val_auc: 0.9363\nEpoch 64/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3406 - accuracy: 0.8517 - auc: 0.9267 - val_loss: 0.3368 - val_accuracy: 0.8593 - val_auc: 0.9363\nEpoch 65/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3405 - accuracy: 0.8496 - auc: 0.9267 - val_loss: 0.3538 - val_accuracy: 0.8443 - val_auc: 0.9327\nEpoch 66/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3369 - accuracy: 0.8496 - auc: 0.9282 - val_loss: 0.3374 - val_accuracy: 0.8486 - val_auc: 0.9362\nEpoch 67/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.8501 - auc: 0.9274 - val_loss: 0.3347 - val_accuracy: 0.8614 - val_auc: 0.9368\nEpoch 68/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3435 - accuracy: 0.8496 - auc: 0.9253 - val_loss: 0.3360 - val_accuracy: 0.8571 - val_auc: 0.9361\nEpoch 69/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3354 - accuracy: 0.8560 - auc: 0.9292 - val_loss: 0.3418 - val_accuracy: 0.8721 - val_auc: 0.9356\nEpoch 70/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3405 - accuracy: 0.8544 - auc: 0.9271 - val_loss: 0.3384 - val_accuracy: 0.8507 - val_auc: 0.9354\nEpoch 71/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3403 - accuracy: 0.8533 - auc: 0.9277 - val_loss: 0.3324 - val_accuracy: 0.8550 - val_auc: 0.9376\nEpoch 72/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3381 - accuracy: 0.8523 - auc: 0.9279 - val_loss: 0.3324 - val_accuracy: 0.8635 - val_auc: 0.9376\nEpoch 73/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3347 - accuracy: 0.8501 - auc: 0.9301 - val_loss: 0.3367 - val_accuracy: 0.8507 - val_auc: 0.9335\nEpoch 74/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3351 - accuracy: 0.8501 - auc: 0.9294 - val_loss: 0.3385 - val_accuracy: 0.8550 - val_auc: 0.9352\nEpoch 75/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.8555 - auc: 0.9287 - val_loss: 0.3349 - val_accuracy: 0.8550 - val_auc: 0.9352\nEpoch 76/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3333 - accuracy: 0.8539 - auc: 0.9299 - val_loss: 0.3309 - val_accuracy: 0.8614 - val_auc: 0.9367\nEpoch 77/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3528 - accuracy: 0.8411 - auc: 0.9215 - val_loss: 0.3583 - val_accuracy: 0.8422 - val_auc: 0.9306\nEpoch 78/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3314 - accuracy: 0.8560 - auc: 0.9309 - val_loss: 0.3310 - val_accuracy: 0.8529 - val_auc: 0.9378\nEpoch 79/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3327 - accuracy: 0.8528 - auc: 0.9305 - val_loss: 0.3316 - val_accuracy: 0.8699 - val_auc: 0.9387\nEpoch 80/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3321 - accuracy: 0.8544 - auc: 0.9309 - val_loss: 0.3322 - val_accuracy: 0.8699 - val_auc: 0.9387\nEpoch 81/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3304 - accuracy: 0.8549 - auc: 0.9316 - val_loss: 0.3344 - val_accuracy: 0.8657 - val_auc: 0.9353\nEpoch 82/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.8592 - auc: 0.9317 - val_loss: 0.3271 - val_accuracy: 0.8529 - val_auc: 0.9391\nEpoch 83/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8523 - auc: 0.9310 - val_loss: 0.3586 - val_accuracy: 0.8571 - val_auc: 0.9376\nEpoch 84/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.8453 - auc: 0.9273 - val_loss: 0.3283 - val_accuracy: 0.8678 - val_auc: 0.9390\nEpoch 85/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3303 - accuracy: 0.8491 - auc: 0.9315 - val_loss: 0.3318 - val_accuracy: 0.8550 - val_auc: 0.9375\nEpoch 86/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3314 - accuracy: 0.8581 - auc: 0.9314 - val_loss: 0.3321 - val_accuracy: 0.8657 - val_auc: 0.9394\nEpoch 87/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3309 - accuracy: 0.8544 - auc: 0.9313 - val_loss: 0.3250 - val_accuracy: 0.8529 - val_auc: 0.9400\nEpoch 88/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3302 - accuracy: 0.8517 - auc: 0.9314 - val_loss: 0.3373 - val_accuracy: 0.8721 - val_auc: 0.9381\nEpoch 89/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3273 - accuracy: 0.8608 - auc: 0.9324 - val_loss: 0.3252 - val_accuracy: 0.8593 - val_auc: 0.9388\nEpoch 90/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3292 - accuracy: 0.8512 - auc: 0.9315 - val_loss: 0.3246 - val_accuracy: 0.8593 - val_auc: 0.9397\nEpoch 91/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8613 - auc: 0.9329 - val_loss: 0.3236 - val_accuracy: 0.8571 - val_auc: 0.9395\nEpoch 92/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3233 - accuracy: 0.8528 - auc: 0.9344 - val_loss: 0.3275 - val_accuracy: 0.8721 - val_auc: 0.9397\nEpoch 93/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3273 - accuracy: 0.8512 - auc: 0.9330 - val_loss: 0.3299 - val_accuracy: 0.8507 - val_auc: 0.9371\nEpoch 94/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.8533 - auc: 0.9331 - val_loss: 0.3228 - val_accuracy: 0.8614 - val_auc: 0.9393\nEpoch 95/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3267 - accuracy: 0.8576 - auc: 0.9330 - val_loss: 0.3300 - val_accuracy: 0.8550 - val_auc: 0.9383\nEpoch 96/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3390 - accuracy: 0.8544 - auc: 0.9262 - val_loss: 0.3286 - val_accuracy: 0.8529 - val_auc: 0.9382\nEpoch 97/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3283 - accuracy: 0.8629 - auc: 0.9316 - val_loss: 0.3230 - val_accuracy: 0.8593 - val_auc: 0.9399\nEpoch 98/100\n54/54 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.8603 - auc: 0.9347 - val_loss: 0.3256 - val_accuracy: 0.8614 - val_auc: 0.9392\nEpoch 99/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8507 - auc: 0.9323 - val_loss: 0.3235 - val_accuracy: 0.8614 - val_auc: 0.9399\nEpoch 100/100\n54/54 [==============================] - 0s 5ms/step - loss: 0.3260 - accuracy: 0.8565 - auc: 0.9333 - val_loss: 0.3214 - val_accuracy: 0.8657 - val_auc: 0.9402\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}