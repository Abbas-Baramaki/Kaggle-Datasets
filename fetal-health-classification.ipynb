{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import compute_class_weight\n\nfrom sklearn.linear_model import LogisticRegression\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-28T07:24:43.135519Z","iopub.execute_input":"2023-10-28T07:24:43.136678Z","iopub.status.idle":"2023-10-28T07:24:43.223175Z","shell.execute_reply.started":"2023-10-28T07:24:43.136643Z","shell.execute_reply":"2023-10-28T07:24:43.222125Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### https://www.kaggle.com/datasets/andrewmvd/fetal-health-classification","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fetal-health-classification/fetal_health.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.506892Z","iopub.execute_input":"2023-10-28T07:23:55.507237Z","iopub.status.idle":"2023-10-28T07:23:55.535951Z","shell.execute_reply.started":"2023-10-28T07:23:55.507202Z","shell.execute_reply":"2023-10-28T07:23:55.535091Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.537462Z","iopub.execute_input":"2023-10-28T07:23:55.538264Z","iopub.status.idle":"2023-10-28T07:23:55.587352Z","shell.execute_reply.started":"2023-10-28T07:23:55.538236Z","shell.execute_reply":"2023-10-28T07:23:55.586146Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"      baseline value  accelerations  fetal_movement  uterine_contractions  \\\n0              120.0          0.000           0.000                 0.000   \n1              132.0          0.006           0.000                 0.006   \n2              133.0          0.003           0.000                 0.008   \n3              134.0          0.003           0.000                 0.008   \n4              132.0          0.007           0.000                 0.008   \n...              ...            ...             ...                   ...   \n2121           140.0          0.000           0.000                 0.007   \n2122           140.0          0.001           0.000                 0.007   \n2123           140.0          0.001           0.000                 0.007   \n2124           140.0          0.001           0.000                 0.006   \n2125           142.0          0.002           0.002                 0.008   \n\n      light_decelerations  severe_decelerations  prolongued_decelerations  \\\n0                   0.000                   0.0                       0.0   \n1                   0.003                   0.0                       0.0   \n2                   0.003                   0.0                       0.0   \n3                   0.003                   0.0                       0.0   \n4                   0.000                   0.0                       0.0   \n...                   ...                   ...                       ...   \n2121                0.000                   0.0                       0.0   \n2122                0.000                   0.0                       0.0   \n2123                0.000                   0.0                       0.0   \n2124                0.000                   0.0                       0.0   \n2125                0.000                   0.0                       0.0   \n\n      abnormal_short_term_variability  mean_value_of_short_term_variability  \\\n0                                73.0                                   0.5   \n1                                17.0                                   2.1   \n2                                16.0                                   2.1   \n3                                16.0                                   2.4   \n4                                16.0                                   2.4   \n...                               ...                                   ...   \n2121                             79.0                                   0.2   \n2122                             78.0                                   0.4   \n2123                             79.0                                   0.4   \n2124                             78.0                                   0.4   \n2125                             74.0                                   0.4   \n\n      percentage_of_time_with_abnormal_long_term_variability  ...  \\\n0                                                  43.0       ...   \n1                                                   0.0       ...   \n2                                                   0.0       ...   \n3                                                   0.0       ...   \n4                                                   0.0       ...   \n...                                                 ...       ...   \n2121                                               25.0       ...   \n2122                                               22.0       ...   \n2123                                               20.0       ...   \n2124                                               27.0       ...   \n2125                                               36.0       ...   \n\n      histogram_min  histogram_max  histogram_number_of_peaks  \\\n0              62.0          126.0                        2.0   \n1              68.0          198.0                        6.0   \n2              68.0          198.0                        5.0   \n3              53.0          170.0                       11.0   \n4              53.0          170.0                        9.0   \n...             ...            ...                        ...   \n2121          137.0          177.0                        4.0   \n2122          103.0          169.0                        6.0   \n2123          103.0          170.0                        5.0   \n2124          103.0          169.0                        6.0   \n2125          117.0          159.0                        2.0   \n\n      histogram_number_of_zeroes  histogram_mode  histogram_mean  \\\n0                            0.0           120.0           137.0   \n1                            1.0           141.0           136.0   \n2                            1.0           141.0           135.0   \n3                            0.0           137.0           134.0   \n4                            0.0           137.0           136.0   \n...                          ...             ...             ...   \n2121                         0.0           153.0           150.0   \n2122                         0.0           152.0           148.0   \n2123                         0.0           153.0           148.0   \n2124                         0.0           152.0           147.0   \n2125                         1.0           145.0           143.0   \n\n      histogram_median  histogram_variance  histogram_tendency  fetal_health  \n0                121.0                73.0                 1.0           2.0  \n1                140.0                12.0                 0.0           1.0  \n2                138.0                13.0                 0.0           1.0  \n3                137.0                13.0                 1.0           1.0  \n4                138.0                11.0                 1.0           1.0  \n...                ...                 ...                 ...           ...  \n2121             152.0                 2.0                 0.0           2.0  \n2122             151.0                 3.0                 1.0           2.0  \n2123             152.0                 4.0                 1.0           2.0  \n2124             151.0                 4.0                 1.0           2.0  \n2125             145.0                 1.0                 0.0           1.0  \n\n[2126 rows x 22 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>baseline value</th>\n      <th>accelerations</th>\n      <th>fetal_movement</th>\n      <th>uterine_contractions</th>\n      <th>light_decelerations</th>\n      <th>severe_decelerations</th>\n      <th>prolongued_decelerations</th>\n      <th>abnormal_short_term_variability</th>\n      <th>mean_value_of_short_term_variability</th>\n      <th>percentage_of_time_with_abnormal_long_term_variability</th>\n      <th>...</th>\n      <th>histogram_min</th>\n      <th>histogram_max</th>\n      <th>histogram_number_of_peaks</th>\n      <th>histogram_number_of_zeroes</th>\n      <th>histogram_mode</th>\n      <th>histogram_mean</th>\n      <th>histogram_median</th>\n      <th>histogram_variance</th>\n      <th>histogram_tendency</th>\n      <th>fetal_health</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>120.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>73.0</td>\n      <td>0.5</td>\n      <td>43.0</td>\n      <td>...</td>\n      <td>62.0</td>\n      <td>126.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>120.0</td>\n      <td>137.0</td>\n      <td>121.0</td>\n      <td>73.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>132.0</td>\n      <td>0.006</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>2.1</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>68.0</td>\n      <td>198.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>141.0</td>\n      <td>136.0</td>\n      <td>140.0</td>\n      <td>12.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>133.0</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>2.1</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>68.0</td>\n      <td>198.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>141.0</td>\n      <td>135.0</td>\n      <td>138.0</td>\n      <td>13.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>134.0</td>\n      <td>0.003</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.003</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>2.4</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>53.0</td>\n      <td>170.0</td>\n      <td>11.0</td>\n      <td>0.0</td>\n      <td>137.0</td>\n      <td>134.0</td>\n      <td>137.0</td>\n      <td>13.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>132.0</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.008</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>2.4</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>53.0</td>\n      <td>170.0</td>\n      <td>9.0</td>\n      <td>0.0</td>\n      <td>137.0</td>\n      <td>136.0</td>\n      <td>138.0</td>\n      <td>11.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2121</th>\n      <td>140.0</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>0.2</td>\n      <td>25.0</td>\n      <td>...</td>\n      <td>137.0</td>\n      <td>177.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>153.0</td>\n      <td>150.0</td>\n      <td>152.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2122</th>\n      <td>140.0</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>78.0</td>\n      <td>0.4</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>103.0</td>\n      <td>169.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>148.0</td>\n      <td>151.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2123</th>\n      <td>140.0</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.007</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>79.0</td>\n      <td>0.4</td>\n      <td>20.0</td>\n      <td>...</td>\n      <td>103.0</td>\n      <td>170.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>153.0</td>\n      <td>148.0</td>\n      <td>152.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2124</th>\n      <td>140.0</td>\n      <td>0.001</td>\n      <td>0.000</td>\n      <td>0.006</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>78.0</td>\n      <td>0.4</td>\n      <td>27.0</td>\n      <td>...</td>\n      <td>103.0</td>\n      <td>169.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>152.0</td>\n      <td>147.0</td>\n      <td>151.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2125</th>\n      <td>142.0</td>\n      <td>0.002</td>\n      <td>0.002</td>\n      <td>0.008</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>74.0</td>\n      <td>0.4</td>\n      <td>36.0</td>\n      <td>...</td>\n      <td>117.0</td>\n      <td>159.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>145.0</td>\n      <td>143.0</td>\n      <td>145.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2126 rows × 22 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['fetal_health'] = data['fetal_health'] - 1","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.588964Z","iopub.execute_input":"2023-10-28T07:23:55.589405Z","iopub.status.idle":"2023-10-28T07:23:55.595248Z","shell.execute_reply.started":"2023-10-28T07:23:55.589368Z","shell.execute_reply":"2023-10-28T07:23:55.594126Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"y = data['fetal_health']\nx = data.drop('fetal_health',axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.597384Z","iopub.execute_input":"2023-10-28T07:23:55.597755Z","iopub.status.idle":"2023-10-28T07:23:55.607797Z","shell.execute_reply.started":"2023-10-28T07:23:55.597729Z","shell.execute_reply":"2023-10-28T07:23:55.606719Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"x.shape\ny.value_counts()\nlen(y.unique())","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.609162Z","iopub.execute_input":"2023-10-28T07:23:55.609484Z","iopub.status.idle":"2023-10-28T07:23:55.621328Z","shell.execute_reply.started":"2023-10-28T07:23:55.609458Z","shell.execute_reply":"2023-10-28T07:23:55.620238Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"3"},"metadata":{}}]},{"cell_type":"code","source":"x_train , x_test , y_train , y_test = train_test_split(x,y,train_size=0.7,random_state=13)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.624672Z","iopub.execute_input":"2023-10-28T07:23:55.624982Z","iopub.status.idle":"2023-10-28T07:23:55.635837Z","shell.execute_reply.started":"2023-10-28T07:23:55.624955Z","shell.execute_reply":"2023-10-28T07:23:55.634747Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class_weights = dict(\n    enumerate(\n        compute_class_weight(\n            'balanced',\n            classes=y.unique(),\n            y=y\n        )\n    \n    )\n\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:23:55.636893Z","iopub.execute_input":"2023-10-28T07:23:55.637247Z","iopub.status.idle":"2023-10-28T07:23:55.647388Z","shell.execute_reply.started":"2023-10-28T07:23:55.637201Z","shell.execute_reply":"2023-10-28T07:23:55.646356Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"inputs=tf.keras.Input(shape =(21,))\nx = tf.keras.layers.Dense(128,activation='relu')(inputs)\nx = tf.keras.layers.Dense(128,activation='relu')(x)\nx = tf.keras.layers.Dense(64,activation='relu')(x)\nx = tf.keras.layers.Dense(16,activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(3,activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs,outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhist = model.fit(\n    x_train,\n    y_train,\n    validation_split = 0.2,\n    batch_size = 25,\n    epochs = 100,\n    callbacks = tf.keras.callbacks.ReduceLROnPlateau()\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:27:11.125483Z","iopub.execute_input":"2023-10-28T07:27:11.125873Z","iopub.status.idle":"2023-10-28T07:27:24.870459Z","shell.execute_reply.started":"2023-10-28T07:27:11.125844Z","shell.execute_reply":"2023-10-28T07:27:24.869271Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1/100\n48/48 [==============================] - 1s 6ms/step - loss: 0.9832 - accuracy: 0.7580 - val_loss: 0.4159 - val_accuracy: 0.8322 - lr: 0.0010\nEpoch 2/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.4995 - accuracy: 0.8101 - val_loss: 0.3310 - val_accuracy: 0.8792 - lr: 0.0010\nEpoch 3/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.8328 - val_loss: 0.4673 - val_accuracy: 0.8389 - lr: 0.0010\nEpoch 4/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.8496 - val_loss: 0.3944 - val_accuracy: 0.8624 - lr: 0.0010\nEpoch 5/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.8244 - val_loss: 0.4441 - val_accuracy: 0.8255 - lr: 0.0010\nEpoch 6/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3904 - accuracy: 0.8395 - val_loss: 0.3908 - val_accuracy: 0.8691 - lr: 0.0010\nEpoch 7/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8563 - val_loss: 0.3060 - val_accuracy: 0.8859 - lr: 0.0010\nEpoch 8/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8487 - val_loss: 0.3193 - val_accuracy: 0.8725 - lr: 0.0010\nEpoch 9/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8454 - val_loss: 0.3572 - val_accuracy: 0.8758 - lr: 0.0010\nEpoch 10/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8479 - val_loss: 0.3611 - val_accuracy: 0.8691 - lr: 0.0010\nEpoch 11/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8513 - val_loss: 0.3136 - val_accuracy: 0.8758 - lr: 0.0010\nEpoch 12/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8496 - val_loss: 0.4327 - val_accuracy: 0.8188 - lr: 0.0010\nEpoch 13/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8462 - val_loss: 0.3514 - val_accuracy: 0.8691 - lr: 0.0010\nEpoch 14/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8496 - val_loss: 0.3283 - val_accuracy: 0.8758 - lr: 0.0010\nEpoch 15/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8613 - val_loss: 0.3041 - val_accuracy: 0.8960 - lr: 0.0010\nEpoch 16/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8370 - val_loss: 0.3502 - val_accuracy: 0.8859 - lr: 0.0010\nEpoch 17/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8471 - val_loss: 0.3540 - val_accuracy: 0.8758 - lr: 0.0010\nEpoch 18/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8597 - val_loss: 0.3022 - val_accuracy: 0.8658 - lr: 0.0010\nEpoch 19/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8571 - val_loss: 0.4199 - val_accuracy: 0.8322 - lr: 0.0010\nEpoch 20/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3474 - accuracy: 0.8571 - val_loss: 0.3259 - val_accuracy: 0.8591 - lr: 0.0010\nEpoch 21/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8622 - val_loss: 0.3049 - val_accuracy: 0.8725 - lr: 0.0010\nEpoch 22/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3340 - accuracy: 0.8639 - val_loss: 0.2989 - val_accuracy: 0.9060 - lr: 0.0010\nEpoch 23/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8580 - val_loss: 0.3084 - val_accuracy: 0.8826 - lr: 0.0010\nEpoch 24/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.8521 - val_loss: 0.3334 - val_accuracy: 0.8389 - lr: 0.0010\nEpoch 25/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8445 - val_loss: 0.3126 - val_accuracy: 0.8691 - lr: 0.0010\nEpoch 26/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3344 - accuracy: 0.8689 - val_loss: 0.2962 - val_accuracy: 0.8792 - lr: 0.0010\nEpoch 27/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8765 - val_loss: 0.3859 - val_accuracy: 0.8658 - lr: 0.0010\nEpoch 28/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8563 - val_loss: 0.3004 - val_accuracy: 0.8826 - lr: 0.0010\nEpoch 29/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3288 - accuracy: 0.8697 - val_loss: 0.2875 - val_accuracy: 0.9027 - lr: 0.0010\nEpoch 30/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3597 - accuracy: 0.8496 - val_loss: 0.3243 - val_accuracy: 0.8591 - lr: 0.0010\nEpoch 31/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.8697 - val_loss: 0.2971 - val_accuracy: 0.8926 - lr: 0.0010\nEpoch 32/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8681 - val_loss: 0.3504 - val_accuracy: 0.8557 - lr: 0.0010\nEpoch 33/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8689 - val_loss: 0.2946 - val_accuracy: 0.8960 - lr: 0.0010\nEpoch 34/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8613 - val_loss: 0.3352 - val_accuracy: 0.8792 - lr: 0.0010\nEpoch 35/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8731 - val_loss: 0.3246 - val_accuracy: 0.8859 - lr: 0.0010\nEpoch 36/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8563 - val_loss: 0.3133 - val_accuracy: 0.8725 - lr: 0.0010\nEpoch 37/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8689 - val_loss: 0.2783 - val_accuracy: 0.9094 - lr: 0.0010\nEpoch 38/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8672 - val_loss: 0.3438 - val_accuracy: 0.8658 - lr: 0.0010\nEpoch 39/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3198 - accuracy: 0.8630 - val_loss: 0.3489 - val_accuracy: 0.8691 - lr: 0.0010\nEpoch 40/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8739 - val_loss: 0.3043 - val_accuracy: 0.9094 - lr: 0.0010\nEpoch 41/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3107 - accuracy: 0.8765 - val_loss: 0.2943 - val_accuracy: 0.8993 - lr: 0.0010\nEpoch 42/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3100 - accuracy: 0.8731 - val_loss: 0.2761 - val_accuracy: 0.9195 - lr: 0.0010\nEpoch 43/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2973 - accuracy: 0.8790 - val_loss: 0.3476 - val_accuracy: 0.8591 - lr: 0.0010\nEpoch 44/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3004 - accuracy: 0.8731 - val_loss: 0.3289 - val_accuracy: 0.8893 - lr: 0.0010\nEpoch 45/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3090 - accuracy: 0.8731 - val_loss: 0.3729 - val_accuracy: 0.8423 - lr: 0.0010\nEpoch 46/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8706 - val_loss: 0.3004 - val_accuracy: 0.8993 - lr: 0.0010\nEpoch 47/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8849 - val_loss: 0.3271 - val_accuracy: 0.8725 - lr: 0.0010\nEpoch 48/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.3126 - accuracy: 0.8840 - val_loss: 0.4224 - val_accuracy: 0.8322 - lr: 0.0010\nEpoch 49/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3262 - accuracy: 0.8681 - val_loss: 0.3091 - val_accuracy: 0.8926 - lr: 0.0010\nEpoch 50/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8773 - val_loss: 0.2959 - val_accuracy: 0.8926 - lr: 0.0010\nEpoch 51/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.8840 - val_loss: 0.2867 - val_accuracy: 0.8826 - lr: 0.0010\nEpoch 52/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2857 - accuracy: 0.8899 - val_loss: 0.2955 - val_accuracy: 0.8960 - lr: 0.0010\nEpoch 53/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8958 - val_loss: 0.2801 - val_accuracy: 0.9094 - lr: 1.0000e-04\nEpoch 54/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.9025 - val_loss: 0.2692 - val_accuracy: 0.9128 - lr: 1.0000e-04\nEpoch 55/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9034 - val_loss: 0.2722 - val_accuracy: 0.9060 - lr: 1.0000e-04\nEpoch 56/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2385 - accuracy: 0.9017 - val_loss: 0.2670 - val_accuracy: 0.9128 - lr: 1.0000e-04\nEpoch 57/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9025 - val_loss: 0.2663 - val_accuracy: 0.9128 - lr: 1.0000e-04\nEpoch 58/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2353 - accuracy: 0.9050 - val_loss: 0.2624 - val_accuracy: 0.9161 - lr: 1.0000e-04\nEpoch 59/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.9084 - val_loss: 0.2670 - val_accuracy: 0.9161 - lr: 1.0000e-04\nEpoch 60/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.9084 - val_loss: 0.2685 - val_accuracy: 0.9161 - lr: 1.0000e-04\nEpoch 61/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2327 - accuracy: 0.9042 - val_loss: 0.2718 - val_accuracy: 0.9094 - lr: 1.0000e-04\nEpoch 62/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9118 - val_loss: 0.2686 - val_accuracy: 0.9195 - lr: 1.0000e-04\nEpoch 63/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9101 - val_loss: 0.2697 - val_accuracy: 0.9060 - lr: 1.0000e-04\nEpoch 64/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.9084 - val_loss: 0.2670 - val_accuracy: 0.9128 - lr: 1.0000e-04\nEpoch 65/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9034 - val_loss: 0.2725 - val_accuracy: 0.9128 - lr: 1.0000e-04\nEpoch 66/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9092 - val_loss: 0.2745 - val_accuracy: 0.9094 - lr: 1.0000e-04\nEpoch 67/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2281 - accuracy: 0.9067 - val_loss: 0.2718 - val_accuracy: 0.9161 - lr: 1.0000e-04\nEpoch 68/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9092 - val_loss: 0.2660 - val_accuracy: 0.9094 - lr: 1.0000e-04\nEpoch 69/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2190 - accuracy: 0.9126 - val_loss: 0.2669 - val_accuracy: 0.9094 - lr: 1.0000e-05\nEpoch 70/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2187 - accuracy: 0.9126 - val_loss: 0.2682 - val_accuracy: 0.9161 - lr: 1.0000e-05\nEpoch 71/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2181 - accuracy: 0.9126 - val_loss: 0.2696 - val_accuracy: 0.9161 - lr: 1.0000e-05\nEpoch 72/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2188 - accuracy: 0.9160 - val_loss: 0.2696 - val_accuracy: 0.9161 - lr: 1.0000e-05\nEpoch 73/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9151 - val_loss: 0.2698 - val_accuracy: 0.9128 - lr: 1.0000e-05\nEpoch 74/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2178 - accuracy: 0.9143 - val_loss: 0.2702 - val_accuracy: 0.9161 - lr: 1.0000e-05\nEpoch 75/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2173 - accuracy: 0.9134 - val_loss: 0.2705 - val_accuracy: 0.9161 - lr: 1.0000e-05\nEpoch 76/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9143 - val_loss: 0.2697 - val_accuracy: 0.9128 - lr: 1.0000e-05\nEpoch 77/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.9134 - val_loss: 0.2712 - val_accuracy: 0.9161 - lr: 1.0000e-05\nEpoch 78/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9134 - val_loss: 0.2705 - val_accuracy: 0.9128 - lr: 1.0000e-05\nEpoch 79/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2164 - accuracy: 0.9134 - val_loss: 0.2706 - val_accuracy: 0.9161 - lr: 1.0000e-06\nEpoch 80/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9143 - val_loss: 0.2706 - val_accuracy: 0.9128 - lr: 1.0000e-06\nEpoch 81/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9143 - val_loss: 0.2706 - val_accuracy: 0.9161 - lr: 1.0000e-06\nEpoch 82/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9126 - val_loss: 0.2706 - val_accuracy: 0.9128 - lr: 1.0000e-06\nEpoch 83/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9134 - val_loss: 0.2707 - val_accuracy: 0.9161 - lr: 1.0000e-06\nEpoch 84/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9143 - val_loss: 0.2707 - val_accuracy: 0.9128 - lr: 1.0000e-06\nEpoch 85/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9126 - val_loss: 0.2706 - val_accuracy: 0.9128 - lr: 1.0000e-06\nEpoch 86/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9143 - val_loss: 0.2708 - val_accuracy: 0.9128 - lr: 1.0000e-06\nEpoch 87/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2163 - accuracy: 0.9126 - val_loss: 0.2707 - val_accuracy: 0.9128 - lr: 1.0000e-06\nEpoch 88/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-06\nEpoch 89/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 90/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 91/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 92/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 93/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 94/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 95/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 96/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 97/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 98/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-07\nEpoch 99/100\n48/48 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-08\nEpoch 100/100\n48/48 [==============================] - 0s 2ms/step - loss: 0.2162 - accuracy: 0.9151 - val_loss: 0.2709 - val_accuracy: 0.9161 - lr: 1.0000e-08\n","output_type":"stream"}]},{"cell_type":"code","source":"model.evaluate(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:27:28.380669Z","iopub.execute_input":"2023-10-28T07:27:28.381141Z","iopub.status.idle":"2023-10-28T07:27:28.495891Z","shell.execute_reply.started":"2023-10-28T07:27:28.381098Z","shell.execute_reply":"2023-10-28T07:27:28.494948Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"20/20 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.9013\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"[0.28617218136787415, 0.9012539386749268]"},"metadata":{}}]},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(x_train,y_train)\nmodel.score(x_test,y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:25:08.701923Z","iopub.execute_input":"2023-10-28T07:25:08.702379Z","iopub.status.idle":"2023-10-28T07:25:08.780766Z","shell.execute_reply.started":"2023-10-28T07:25:08.702344Z","shell.execute_reply":"2023-10-28T07:25:08.779783Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.877742946708464"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}